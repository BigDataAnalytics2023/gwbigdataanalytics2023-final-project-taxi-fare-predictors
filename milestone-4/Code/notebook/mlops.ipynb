{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the latest sagemaker and boto3 SDKs.\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.1.0<3\" tqdm\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==2.0.0\"\n",
    "!{sys.executable} -m pip show sagemaker stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart your SageMaker kernel then continue with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace `None` with the project name when creating SageMaker Project\n",
    "# You can find it from the left panel in Studio\n",
    "\n",
    "PROJECT_NAME = None\n",
    "\n",
    "assert PROJECT_NAME is not None and isinstance(\n",
    "    PROJECT_NAME, str\n",
    "), \"Please specify the project name as string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "\n",
    "def get_provisioned_product_name(project_name):\n",
    "    region = boto3.Session().region_name\n",
    "    sc = boto3.client(\"servicecatalog\")\n",
    "    products = sc.search_provisioned_products(\n",
    "        Filters={\n",
    "            \"SearchQuery\": [\n",
    "                project_name,\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    pp = products[\"ProvisionedProducts\"]\n",
    "    if len(pp) != 1:\n",
    "        print(\"Invalid provisioned product name. Open the link below and search manually\")\n",
    "        display(\n",
    "            HTML(\n",
    "                f'<a target=\"_blank\" href=\"https://{region}.console.aws.amazon.com/servicecatalog/home?region={region}#provisioned-products\">Service Catalog</a>'\n",
    "            )\n",
    "        )\n",
    "        raise ValueError(\"Invalid provisioned product\")\n",
    "\n",
    "    return pp[0][\"Name\"]\n",
    "\n",
    "\n",
    "PROVISIONED_PRODUCT_NAME = get_provisioned_product_name(PROJECT_NAME)\n",
    "print(\n",
    "    f\"The associated Service Catalog Provisioned Product Name to this SagaMaker project: {PROVISIONED_PRODUCT_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of any errors, you can examine the Service Catalog console from the above link and find the associated provisioned product name which is something like `example-p-1v7hbpwe594n` and assigns it to `PROVISIONED_PRODUCT_NAME` manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    " \n",
    "In this section of the notebook, you will download the publicly available New York Taxi dataset in preparation for uploading it to S3.\n",
    "\n",
    "### Download Dataset\n",
    "\n",
    "First, download a sample of the New York City Taxi [dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/)â‡— to this notebook instance. This dataset contains information on trips taken by taxis and for-hire vehicles in New York City, including pick-up and drop-off times and locations, fares, distance traveled, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://nyc-tlc/trip data/green_tripdata_2018-02.csv' 'nyc-tlc.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the dataset into a pandas data frame, taking care to parse the dates correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parse_dates = [\"lpep_dropoff_datetime\", \"lpep_pickup_datetime\"]\n",
    "trip_df = pd.read_csv(\"nyc-tlc.csv\", parse_dates=parse_dates)\n",
    "\n",
    "trip_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation\n",
    "\n",
    "Instead of the raw date and time features for pick-up and drop-off, let's use these features to calculate the total time of the trip in minutes, which will be easier to work with for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df[\"duration_minutes\"] = (\n",
    "    trip_df[\"lpep_dropoff_datetime\"] - trip_df[\"lpep_pickup_datetime\"]\n",
    ").dt.seconds / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a lot of columns we don't need, so let's select a sample of columns for our machine learning model. Keep only `total_amount` (fare), `duration_minutes`, `passenger_count`, and `trip_distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"total_amount\", \"duration_minutes\", \"passenger_count\", \"trip_distance\"]\n",
    "data_df = trip_df[cols]\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some quick statistics for the dataset to understand the quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows some clear outliers, e.g. -400 or 2626 as fare, or 0 passengers. There are many intelligent methods for identifying and removing outliers, but data cleaning is not the focus of this notebook, so just remove the outliers by setting some min and max values which seem more reasonable. Removing the outliers results in a final dataset of 754,671 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[\n",
    "    (data_df.total_amount > 0)\n",
    "    & (data_df.total_amount < 200)\n",
    "    & (data_df.duration_minutes > 0)\n",
    "    & (data_df.duration_minutes < 120)\n",
    "    & (data_df.trip_distance > 0)\n",
    "    & (data_df.trip_distance < 121)\n",
    "    & (data_df.passenger_count > 0)\n",
    "].dropna()\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "Since this notebook will build a regression model for the taxi data, it's a good idea to check if there is any correlation between the variables in our data. Use scatter plots on a sample of the data to compare trip distance with duration in minutes, and total amount (fare) with duration in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sample_df = data_df.sample(1000)\n",
    "sns.scatterplot(data=sample_df, x=\"duration_minutes\", y=\"trip_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=sample_df, x=\"duration_minutes\", y=\"total_amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scatter plots look fine and show at least some correlation between our variables. \n",
    "\n",
    "### Data splitting and saving\n",
    "\n",
    "We are now ready to split the dataset into train, validation, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.20, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.05, random_state=42)\n",
    "\n",
    "# Reset the index for our test dataframe\n",
    "test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\n",
    "    \"Size of\\n train: {},\\n val: {},\\n test: {} \".format(\n",
    "        train_df.shape[0], val_df.shape[0], test_df.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = [\"total_amount\", \"duration_minutes\", \"passenger_count\", \"trip_distance\"]\n",
    "train_df.to_csv(\"train.csv\", index=False, header=False)\n",
    "val_df.to_csv(\"validation.csv\", index=False, header=False)\n",
    "test_df.to_csv(\"test.csv\", index=False, header=False)\n",
    "\n",
    "# Save test and baseline with headers\n",
    "train_df.to_csv(\"baseline.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload these CSV files to your default SageMaker S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Get the session and default bucket\n",
    "session = sagemaker.session.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# Specify data prefix and version\n",
    "prefix = \"nyc-tlc/v1\"\n",
    "\n",
    "s3_train_uri = session.upload_data(\"train.csv\", bucket, prefix + \"/data/training\")\n",
    "s3_val_uri = session.upload_data(\"validation.csv\", bucket, prefix + \"/data/validation\")\n",
    "s3_test_uri = session.upload_data(\"test.csv\", bucket, prefix + \"/data/test\")\n",
    "s3_baseline_uri = session.upload_data(\"baseline.csv\", bucket, prefix + \"/data/baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the datasets which you have prepared and saved in this section to trigger the pipeline to train and deploy a model in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def get_config(provisioned_product_name):\n",
    "    sc = boto3.client(\"servicecatalog\")\n",
    "    outputs = sc.get_provisioned_product_outputs(ProvisionedProductName=provisioned_product_name)[\n",
    "        \"Outputs\"\n",
    "    ]\n",
    "    config = {}\n",
    "    for out in outputs:\n",
    "        config[out[\"OutputKey\"]] = out[\"OutputValue\"]\n",
    "    return config\n",
    "\n",
    "\n",
    "config = get_config(PROVISIONED_PRODUCT_NAME)\n",
    "region = config[\"Region\"]\n",
    "artifact_bucket = config[\"ArtifactBucket\"]\n",
    "pipeline_name = config[\"PipelineName\"]\n",
    "model_name = config[\"ModelName\"]\n",
    "workflow_pipeline_arn = config[\"WorkflowPipelineARN\"]\n",
    "\n",
    "print(\"region: {}\".format(region))\n",
    "print(\"artifact bucket: {}\".format(artifact_bucket))\n",
    "print(\"pipeline: {}\".format(pipeline_name))\n",
    "print(\"model name: {}\".format(model_name))\n",
    "print(\"workflow: {}\".format(workflow_pipeline_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "input_data = {\n",
    "    \"TrainingUri\": s3_train_uri,\n",
    "    \"ValidationUri\": s3_val_uri,\n",
    "    \"TestUri\": s3_test_uri,\n",
    "    \"BaselineUri\": s3_baseline_uri,\n",
    "}\n",
    "\n",
    "hyperparameters = {\"num_round\": 50}\n",
    "\n",
    "zip_buffer = BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, \"a\") as zf:\n",
    "    zf.writestr(\"inputData.json\", json.dumps(input_data))\n",
    "    zf.writestr(\"hyperparameters.json\", json.dumps(hyperparameters))\n",
    "zip_buffer.seek(0)\n",
    "\n",
    "data_source_key = \"{}/data-source.zip\".format(pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload the zip package to your artifact S3 bucket - this action will trigger the pipeline to train and deploy a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "s3.put_object(Bucket=artifact_bucket, Key=data_source_key, Body=bytearray(zip_buffer.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\n",
    "    '<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{1}/view?region={0}\">Code Pipeline</a>'.format(\n",
    "        region, pipeline_name\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codepipeline = boto3.client(\"codepipeline\")\n",
    "\n",
    "\n",
    "def get_pipeline_stage(pipeline_name, stage_name):\n",
    "    response = codepipeline.get_pipeline_state(name=pipeline_name)\n",
    "    for stage in response[\"stageStates\"]:\n",
    "        if stage[\"stageName\"] == stage_name:\n",
    "            return stage\n",
    "\n",
    "\n",
    "# Get last execution id\n",
    "build_stage = get_pipeline_stage(pipeline_name, \"Build\")\n",
    "if not \"latestExecution\" in build_stage:\n",
    "    raise (Exception(\"Please wait.  Build not started\"))\n",
    "\n",
    "build_url = build_stage[\"actionStates\"][0][\"latestExecution\"][\"externalExecutionUrl\"]\n",
    "\n",
    "# Out a link to the code build logs\n",
    "HTML('<a target=\"_blank\" href=\"{0}\">Code Build Logs</a>'.format(build_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AWS CodeBuild process is responsible for creating a number of AWS CloudFormation templates which we will explore in more detail in the next section.  Two of these templates are used to set up the **Train** step by creating the AWS Step Functions worklow and the custom AWS Lambda functions used within this workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        workflow = Workflow.attach(workflow_pipeline_arn)\n",
    "        break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)\n",
    "\n",
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store input_data PROVISIONED_PRODUCT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Analytics\n",
    "\n",
    "Once the training and baseline jobs are complete (meaning they are displayed in a green color in the Step Functions workflow, this takes around 5 minutes), you can inspect the experiment metrics. The code below will display all experiments in a table. Note that the baseline processing job won't have RMSE metrics - it calculates metrics based on the training data, but does not train a machine learning model. \n",
    "\n",
    "You will [explore the baseline](#Explore-Baseline) results later in this notebook. <a id=\"validation-results\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import analytics\n",
    "\n",
    "experiment_name = \"mlops-{}\".format(model_name)\n",
    "model_analytics = analytics.ExperimentAnalytics(experiment_name=experiment_name)\n",
    "analytics_df = model_analytics.dataframe()\n",
    "\n",
    "if analytics_df.shape[0] == 0:\n",
    "    raise (Exception(\"Please wait.  No training or baseline jobs\"))\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 100)  # Increase column width to show full copmontent name\n",
    "cols = [\n",
    "    \"TrialComponentName\",\n",
    "    \"DisplayName\",\n",
    "    \"SageMaker.InstanceType\",\n",
    "    \"train:rmse - Last\",\n",
    "    \"validation:rmse - Last\",\n",
    "]  # return the last rmse for training and validation\n",
    "analytics_df[analytics_df.columns & cols].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Dev\n",
    "\n",
    "### Test Dev Deployment\n",
    "\n",
    "When the pipeline has finished training a model, it automatically moves to the next step, where the model is deployed as a SageMaker Endpoint. This endpoint is part of your dev deployment, therefore, in this section, you will run some tests on the endpoint to decide if you want to deploy this model into production.\n",
    "\n",
    "First, run the cell below to fetch the name of the SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codepipeline = boto3.client(\"codepipeline\")\n",
    "\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, \"DeployDev\")\n",
    "if not \"latestExecution\" in deploy_dev:\n",
    "    raise (Exception(\"Please wait.  Deploy dev not started\"))\n",
    "\n",
    "execution_id = deploy_dev[\"latestExecution\"][\"pipelineExecutionId\"]\n",
    "dev_endpoint_name = \"mlops-{}-dev-{}\".format(model_name, execution_id)\n",
    "\n",
    "print(\"endpoint name: {}\".format(dev_endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sm.describe_endpoint(EndpointName=dev_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response[\"EndpointStatus\"]))\n",
    "        if response[\"EndpointStatus\"] == \"InService\":\n",
    "            break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your endpoint is ready, let's write some code to run the test data (which you split off from the dataset and saved to file at the start of this notebook) through the endpoint for inference. The code below supports both v1 and v2 of the SageMaker SDK, but we recommend using v2 of the SDK in all of your future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "\n",
    "def get_predictor(endpoint_name):\n",
    "    xgb_predictor = Predictor(endpoint_name)\n",
    "    xgb_predictor.serializer = CSVSerializer()\n",
    "    return xgb_predictor\n",
    "\n",
    "\n",
    "def predict(predictor, data, rows=500):\n",
    "    split_array = np.array_split(data, round(data.shape[0] / float(rows)))\n",
    "    predictions = \"\"\n",
    "    for array in tqdm(split_array):\n",
    "        predictions = \",\".join([predictions, predictor.predict(array).decode(\"utf-8\")])\n",
    "    return np.fromstring(predictions[1:], sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the `predict` function, which was defined in the code above, to run the test data through the endpoint and generate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predictor = get_predictor(dev_endpoint_name)\n",
    "predictions = predict(dev_predictor, test_df[test_df.columns[1:]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the predictions into a data frame, and join it with your test data. Then, calculate absolute error as the difference between the actual taxi fare and the predicted taxi fare. Display the results in a table, sorted by the highest absolute error values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"total_amount_predictions\": predictions})\n",
    "pred_df = test_df.join(pred_df)  # Join on all\n",
    "pred_df[\"error\"] = abs(pred_df[\"total_amount\"] - pred_df[\"total_amount_predictions\"])\n",
    "\n",
    "pred_df.sort_values(\"error\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table, we note that some short trip distances have large errors because the low predicted fare does not match the high actual fare. This could be the result of a generous tip which we haven't included in this dataset.\n",
    "\n",
    "You can also analyze the results by plotting the absolute error to visualize outliers. In this graph, we see that most of the outliers are cases where the model predicted a much lower fare than the actual fare. There are only a few outliers where the model predicted a higher fare than the actual fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=pred_df, x=\"total_amount_predictions\", y=\"total_amount\", hue=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want one overall measure of quality for the model, you can calculate the root mean square error (RMSE) for the predicted fares compared to the actual fares. Compare this to the [results calculated on the validation set](#validation-results) at the end of the 'Inspect Training Job' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def rmse(pred_df):\n",
    "    return sqrt(mean_squared_error(pred_df[\"total_amount\"], pred_df[\"total_amount_predictions\"]))\n",
    "\n",
    "\n",
    "print(\"RMSE: {}\".format(rmse(pred_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def accuracy(pred_df):\n",
    "    return r2_score(pred_df[\"total_amount\"], pred_df[\"total_amount_predictions\"])\n",
    "\n",
    "print(\"Accuracy (R-squared): {}\".format(accuracy(pred_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Prod\n",
    "\n",
    "### Approve Deployment to Production\n",
    "\n",
    "If you are happy with the results of the model, you can go ahead and approve the model to be deployed into production. You can do so by clicking the **Review** button in the CodePipeline UI, leaving a comment to explain why you approve this model, and clicking on **Approve**. \n",
    "\n",
    "Alternatively, you can create a Jupyter widget which (when enabled) allows you to comment and approve the model directly from this notebook. Run the cell below to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "def on_click(obj):\n",
    "    result = {\"summary\": approval_text.value, \"status\": obj.description}\n",
    "    response = codepipeline.put_approval_result(\n",
    "        pipelineName=pipeline_name,\n",
    "        stageName=\"DeployDev\",\n",
    "        actionName=\"ApproveDeploy\",\n",
    "        result=result,\n",
    "        token=approval_action[\"token\"],\n",
    "    )\n",
    "    button_box.close()\n",
    "    print(result)\n",
    "\n",
    "\n",
    "# Create the widget if we are ready for approval\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, \"DeployDev\")\n",
    "if not \"latestExecution\" in deploy_dev[\"actionStates\"][-1]:\n",
    "    raise (Exception(\"Please wait.  Deploy dev not complete\"))\n",
    "\n",
    "approval_action = deploy_dev[\"actionStates\"][-1][\"latestExecution\"]\n",
    "if approval_action[\"status\"] == \"Succeeded\":\n",
    "    print(\"Dev approved: {}\".format(approval_action[\"summary\"]))\n",
    "elif \"token\" in approval_action:\n",
    "    approval_text = widgets.Text(placeholder=\"Optional approval message\")\n",
    "    approve_btn = widgets.Button(description=\"Approved\", button_style=\"success\", icon=\"check\")\n",
    "    reject_btn = widgets.Button(description=\"Rejected\", button_style=\"danger\", icon=\"close\")\n",
    "    approve_btn.on_click(on_click)\n",
    "    reject_btn.on_click(on_click)\n",
    "    button_box = widgets.HBox([approval_text, approve_btn, reject_btn])\n",
    "    display(button_box)\n",
    "else:\n",
    "    raise (Exception(\"Please wait. No dev approval\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Production Deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_prd = get_pipeline_stage(pipeline_name, \"DeployPrd\")\n",
    "if not \"latestExecution\" in deploy_prd or not \"latestExecution\" in deploy_prd[\"actionStates\"][0]:\n",
    "    raise (Exception(\"Please wait.  Deploy prd not started\"))\n",
    "\n",
    "execution_id = deploy_prd[\"latestExecution\"][\"pipelineExecutionId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "\n",
    "def get_event_dataframe(events):\n",
    "    stack_cols = [\n",
    "        \"LogicalResourceId\",\n",
    "        \"ResourceStatus\",\n",
    "        \"ResourceStatusReason\",\n",
    "        \"Timestamp\",\n",
    "    ]\n",
    "    stack_event_df = pd.DataFrame(events)[stack_cols].fillna(\"\")\n",
    "    stack_event_df[\"TimeAgo\"] = datetime.now(tzlocal()) - stack_event_df[\"Timestamp\"]\n",
    "    return stack_event_df.drop(\"Timestamp\", axis=1)\n",
    "\n",
    "\n",
    "cfn = boto3.client(\"cloudformation\")\n",
    "\n",
    "stack_name = stack_name = \"{}-deploy-prd\".format(pipeline_name)\n",
    "print(\"stack name: {}\".format(stack_name))\n",
    "\n",
    "# Get latest stack events\n",
    "while True:\n",
    "    try:\n",
    "        response = cfn.describe_stack_events(StackName=stack_name)\n",
    "        break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)\n",
    "\n",
    "get_event_dataframe(response[\"StackEvents\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to fetch the name of the endpoint, then run a loop to wait for the endpoint to be fully deployed. You need the status to be 'InService'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_endpoint_name = \"mlops-{}-prd-{}\".format(model_name, execution_id)\n",
    "print(\"prod endpoint: {}\".format(prd_endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sm.describe_endpoint(EndpointName=prd_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response[\"EndpointStatus\"]))\n",
    "        # Wait until the endpoint is in service with data capture enabled\n",
    "        if (\n",
    "            response[\"EndpointStatus\"] == \"InService\"\n",
    "            and \"DataCaptureConfig\" in response\n",
    "            and response[\"DataCaptureConfig\"][\"EnableCapture\"]\n",
    "        ):\n",
    "            break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the endpoint status is 'InService', you can continue. Earlier in this notebook, you created some code to send data to the dev endpoint. Reuse this code now to send a sample of the test data to the production endpoint. Since data capture is enabled on this endpoint, you want to send single records at a time, so the model monitor can map these records to the baseline. \n",
    "\n",
    "You will [inspect the model monitor](#Inspect-Model-Monitor) later in this notebook. For now, just check if you can send data to the endpoint and receive predictions in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_predictor = get_predictor(prd_endpoint_name)\n",
    "sample_values = test_df[test_df.columns[1:]].sample(100).values\n",
    "predictions = predict(prd_predictor, sample_values, rows=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Execute the following cell to delete the stacks created in the pipeline. For a model name of **nyctaxi** these would be:\n",
    "\n",
    "1. *nyctaxi*-deploy-prd\n",
    "2. *nyctaxi*-deploy-dev\n",
    "3. *nyctaxi*-workflow\n",
    "4. sagemaker-custom-resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfn = boto3.client(\"cloudformation\")\n",
    "\n",
    "# Delete the prod and then dev stack\n",
    "for stack_name in [\n",
    "    f\"{pipeline_name}-deploy-prd\",\n",
    "    f\"{pipeline_name}-deploy-dev\",\n",
    "    f\"{pipeline_name}-workflow\",\n",
    "    f\"mlops-{model_name}-{config['SageMakerProjectId']}-sagemaker-custom-resource\",\n",
    "]:\n",
    "    print(\"Deleting stack: {}\".format(stack_name))\n",
    "    cfn.delete_stack(StackName=stack_name)\n",
    "    cfn.get_waiter(\"stack_delete_complete\").wait(StackName=stack_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will delete the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudwatch.delete_dashboards(DashboardNames=[dashboard_name])\n",
    "print(\"Dashboard deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will clean up all objects in the artifact bucket and delete the SageMaker project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3')\n",
    "s3_artifact_bucket = s3_resource.Bucket(artifact_bucket)\n",
    "s3_artifact_bucket.object_versions.delete()\n",
    "print(\"Artifact bucket objects deleted\")\n",
    "\n",
    "sm.delete_project(\n",
    "    ProjectName=PROJECT_NAME\n",
    ")\n",
    "print(\"SageMaker Project deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, close this notebook."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
